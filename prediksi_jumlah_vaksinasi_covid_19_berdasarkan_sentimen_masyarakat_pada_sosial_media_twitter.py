# -*- coding: utf-8 -*-
"""Prediksi Jumlah Vaksinasi COVID-19 Berdasarkan Sentimen Masyarakat pada Sosial Media Twitter

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1wkdaekWUEci5qCG5jf7wQwlKBlLv7Tka

#### Instalasi
Opsional
"""

!pip install emoji
!pip3 install snscrape
!pip3 install numpy
!pip3 install pandas

!pip install nltk
!pip install PySastrawi

import nltk
nltk.download('punkt')

"""#### Import Library"""

# import
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from sklearn import preprocessing
from keras.models import Sequential
from keras.layers import Dense
import re
import emoji
import seaborn as sn
import random
import requests
import numpy as np
import snscrape.modules.twitter as sntwitter
import itertools

from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
import matplotlib.pyplot as plt
from nltk.tokenize import word_tokenize
from wordcloud import WordCloud, STOPWORDS

"""#### Import Data"""

#Data Jumlah Vaksinasi

#Proses labeling data Twitter berdasarkan sentimen yang dihasilkan dari setiap text tweet dilakukan secara manual. 
vaksin = pd.read_csv('data_tweet.csv', sep=';', encoding= 'unicode_escape')

#Data Jumlah Vaksinasi
#Read data
dat = pd.read_csv('Indonesia.csv')
dat

"""#### Data Preparation

##### a.Data Twitter
"""

vaksin = vaksin.rename(columns={'Ã¯Â»Â¿id': 'id'})
vaksin

## Cleansing Data

# 1. Membersihkan data tanggal menjadi format yang benar
vaksin[['created','timestamp']] = vaksin.created_at.str.split("T",expand=True,)
vaksin

#Data tanggal tidak beraturan, dilakukan pemisahan atas tanggal dan waktu vaksinasi
#Yang digunakan dalam analisis hanya tanggal saja

# 2. Menghapus kolom yang tidak relevan untuk analisis

vaksin_new = vaksin.drop(columns=['created_at', 'timestamp', 'id'])

vaksin_new.rename(columns={'created': 'created_at'}, inplace=True)

vaksin_new = vaksin_new.reindex(columns=['created_at', 'text', 'label'])
vaksin_new['created_at']= pd.to_datetime(vaksin_new['created_at'])

# Hasil pembersihan data tanggal
vaksin_new

#3. Check NA Value + membersihkan NA

#Mencari NA dalam dataframe

vaksin_new.isnull().sum()

#Terdapat satu NA pada kolom label

#Melihat data yang memiliki nilai NA

vaksin_new[vaksin_new['label'].isnull()]

#Replace NA with value

vaksin_new.loc[vaksin_new.index[2172], 'label'] = 1

#Opsi lain: row dengan NA tersebut dihapus
#Namun karena jelas bahwa komentar tersebut adalah komentar positif, maka diputuskan untuk menambahkan label manual saja dibandingkan menghapus barisnya

#Change label type to integer
vaksin_new['label'] = vaksin_new['label'].astype(int)

# 4. Pembersihan enter menjadi spasi dan karakter yang tidak diperlukan

# Mengubah enter menjadi spasi
vaksin_new['text'] = vaksin_new['text'].replace({r'\s+$': '', r'^\s+': ''}, regex=True).replace(r'\n',  ' ', regex=True)

# Menghapus RT, url, @, hashtag, emoji, selain angka sama huruf

def cleaner(tweet):
    tweet = re.sub("@[A-Za-z0-9]+","",tweet) #Remove @ sign
    tweet = re.sub(r"(?:\@|http?\://|https?\://|www)\S+", "", tweet) #Remove http links
    tweet = " ".join(tweet.split())
    tweet = ''.join(c for c in tweet if c not in emoji.UNICODE_EMOJI) #Remove Emojis
    tweet = tweet.replace("#", "").replace("_", " ") #Remove hashtag sign but keep the text
    tweet = tweet.replace("RT", "")
    tweet = tweet.replace(":", "")
    tweet = re.sub('[^A-Za-z0-9 ]+', '', tweet)
    return tweet
vaksin_new['text'] = vaksin_new['text'].map(lambda x: cleaner(x))

# Menghapus tweet duplikat
vaksin_new.drop_duplicates(subset ="text", keep = False, inplace = True)

vaksin_new

vaksin_new.info()

# Ubah label jadi karakter
vaksin_new.loc[vaksin_new.label == -1, 'label'] = 'negative'
vaksin_new.loc[vaksin_new.label == 0, 'label'] = 'neutral'
vaksin_new.loc[vaksin_new.label == 1, 'label'] = 'positive'

# One Hot Encoding
label = pd.get_dummies(vaksin_new.label)

label

# menggabungkan data vaksin_new dan label
vaksin_final = pd.concat([vaksin_new, label], axis=1)

#drop label lama
vaksin_final = vaksin_final.drop(columns=['label'])

vaksin_final

# Mengubah ke format week dan menjumlahkan sentimen berdasarkan hitungan minggu

vaksin_final['Week'] = vaksin_final['created_at'].dt.week

negatif = vaksin_final.groupby(['Week'], as_index=False)[['negative']].sum()

netral = vaksin_final.groupby(['Week'], as_index=False)[['neutral']].sum()
netral = netral.drop(columns=['Week'])

positif = vaksin_final.groupby(['Week'], as_index=False)[['positive']].sum()
positif = positif.drop(columns=['Week'])

vaksin_final = pd.concat([negatif, netral, positif], axis=1)

vaksin_final[2:3] = vaksin_final[2:3].astype(int)
vaksin_final

#Preprocessing data tweet selesai

"""##### b. Data Jumlah Vaksinasi"""

dat

# Cleansing Data

#Ubah tipe data date
dat['date']= pd.to_datetime(dat['date'])
dat['total_vaccinations'] = dat['total_vaccinations'].astype(int)

dat.info()

#Filtering data
start_date = "2021-03-01"
end_date = "2021-08-30"

after_start_date = dat["date"] >= start_date
before_end_date = dat["date"] <= end_date
between_two_dates = after_start_date & before_end_date
filtered_dates = dat.loc[between_two_dates]

filtered_dates

#Filter column
dat_new = filtered_dates.drop(columns=['location', 'vaccine', 'source_url', 'people_vaccinated', 'people_fully_vaccinated', 'total_boosters'])

dat_new

# Mengubah ke format mingguan (vaksin ditotalkan perminggu)
dat_new['Week'] = dat_new['date'].dt.week
total_vaksin = dat_new.groupby(['Week'], as_index=False)[['total_vaccinations']].sum()
total_vaksin

# Menggabungkan data twitter & vaksin
total_vaksin = total_vaksin.drop(columns = ['Week'], axis = 1)
final = pd.concat([vaksin_final, total_vaksin], axis=1)
final

"""#### Text Processing"""

# Case Folding Data Tweet

vaksin_new["text"] = vaksin_new["text"].str.lower()

tweet = vaksin_new['text'].str.cat(sep='')
tweet

# Tokenization 

tweet_tokens = word_tokenize(tweet)
tweet_tokens

# Menghilangkan Stopwords

content = open("stopwords.txt")
file_contents = content.read()
stopword = file_contents.splitlines()

stopword

# World Cloud

wordcloud = WordCloud(background_color ='white', stopwords = stopword).generate(str(tweet))

plt.imshow(wordcloud)
plt.axis("off")
plt.show()

"""#### Analisis Korelasi Data"""

# Eksplorasi data
final[['negative','neutral','positive','total_vaccinations']].corr()

# Correlation Matrix
final_corr = final[['negative', 'neutral', 'positive', 'total_vaccinations']]
corrMatrix = final_corr.corr()
sn.heatmap(corrMatrix, annot=True)
plt.show()

"""#### Data Modeling: Prediction with Linear Regression

Model ini dibentuk untuk memprediksi jumlah vaksinasi COVID-19 berdasarkan sentimen opini masyarakat pada sosial media Twitter.
"""

from sklearn.linear_model import LinearRegression
import statsmodels.api as sm

x = final[['negative',	'neutral',	'positive']]
y = final['total_vaccinations']

model = LinearRegression().fit(x, y)
r_sq = model.score(x, y)
print('coefficient of determination:', r_sq)

new_sentimen_test = {'negative': [10, 10], 'neutral': [90, 30], 'positive': [120, 10]}
new_sentimen_test = pd.DataFrame(new_sentimen_test)

lr_pred = model.predict(new_sentimen_test)
print('Hasil prediksi:', lr_pred)